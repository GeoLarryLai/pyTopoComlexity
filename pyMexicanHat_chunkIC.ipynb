{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topographic Complexity/Variability: Mexican Hat Wavelet Analysis  \n",
    "* The original MATLAB code was developed by Dr. Adam M. Booth (Portland State Univeristy).  \n",
    "    * Citations:  \n",
    "        * Booth, A.M., Roering, J.J., Perron, J.T., 2009. Automated landslide mapping using spectral analysis and high-resolution topographic data: Puget Sound lowlands, Washington, and Portland    Hills, Oregon. Geomorphology 109, 132-147. https://doi.org/10.1016/j.geomorph.2009.02.027  \n",
    "        * Booth, A.M., LaHusen, S.R., Duvall, A.R., Montgomery, D.R., 2017. Holocene history of deep-seated landsliding in the North Fork Stillaguamish River valley from surface roughness analysis, radiocarbon dating, and numerical landscape evolution modeling. Journal of Geophysical Research: Earth Surface 122, 456-472. https://doi.org/10.1002/2016JF003934  \n",
    "* This MATLAB code was later adapted by Dr. Sean R. LaHusen (Univeristy of Washington) & revised by Erich N. Herzig (Univeristy of Washington).\n",
    "    * Citations:  \n",
    "       * LaHusen, S.R., Duvall, A.R., Booth, A.M., Montgomery, D.R., 2016. Surface roughness dating of long-runout landslides near Oso, Washington (USA), reveals persistent postglacial hillslope instability. Geology 44, 111-114. https://doi.org/10.1130/G37267.1  \n",
    "       * LaHusen, S.R., Duvall, A.R., Booth, A.M., Grant, A., Mishkin, B.A., Montgomery, D.R., Struble, W., Roering, J.J., Wartman, J., 2020. Rainfall triggers more deep-seated landslides than Cascadia earthquakes in the Oregon Coast Range, USA. Science Advances 6, eaba6790. https://doi.org/10.1126/sciadv.aba6790  \n",
    "       * Herzig, E.N., Duvall, A.R., Booth, A.R., Stone, I., Wirth, E., LaHusen, S.R., Wartman, J., Grant, A.; Evidence of Seattle Fault Earthquakes from Patterns in Deep‐Seated Landslides. Bulletin of the Seismological Society of America 2023; https://doi.org/10.1785/0120230079 \n",
    "* In November, 2023; this is code translated and optimized into this python version by Dr. Larry Syu-Heng Lai (Univeristy of Washington)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.signal import fftconvolve\n",
    "import rasterio\n",
    "import dask.array as da\n",
    "from tqdm.dask import TqdmCallback  # Import TqdmCallback for Dask integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mexican function using FFT Colvolve\n",
    "\"\"\"\n",
    "    Computes the 2D CWT of dem using the Mexican Hat wavelet. \n",
    "    Optimized using fft\n",
    "    \n",
    "    dem = digital elevation model\n",
    "    a = wavelet scale\n",
    "    dx = grid spacing (same in x- and y-directions)\n",
    "\n",
    "    Returns:\n",
    "    C = array of wavelet coefficients, indexed to dem\n",
    "    frq = bandpass frequency of wavelet at scale a\n",
    "    wave = wavelength (inverse of frq)\n",
    "\"\"\"\n",
    "In this version, masking the artifacts at the edges are conducted during chunk processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2_mexh_fft(dem, a, dx):\n",
    "    # Generate the mexican hat wavelet kernel at wavelet scale a.\n",
    "    # The kernel must be large enough for the wavelet to decay to ~0 at the edges.\n",
    "    X, Y = np.meshgrid(np.arange(-8*a, 8*a+1), np.arange(-8*a, 8*a+1))\n",
    "\n",
    "    # This psi has been scaled, so C is equal to curvature:\n",
    "    psi = (-1/(np.pi*(a*dx)**4)) * (1 - (X**2 + Y**2)/(2*a**2)) * np.exp(-(X**2 + Y**2)/(2*a**2))  # units of [1/(m^4)]\n",
    "\n",
    "    # Convolve dem with psi using scipy's convolve2d function, multiplying by dx^2\n",
    "    # to approximate the double integral. 'same' mode crops C to same size as dem.\n",
    "    C = (dx**2) * fftconvolve(dem*0.3048, psi, mode='same')  # units of [(m^2) x (m) x (1/(m^4)) = (1/m)]\n",
    "\n",
    "    # Frequency and wavelength vectors:\n",
    "    wave = 2*np.pi*dx*a/(5/2)**(1/2)  # Torrence and Compo [1998]\n",
    "    frq = 1./wave\n",
    "\n",
    "    return C, frq, wave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup files directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "base_dir = '.../' #put your own directory\n",
    "#input_dir = os.path.join(base_dir, 'Test_DEM.tif')\n",
    "#output_dir = os.path.join(base_dir, 'Test_pymexhat_chunk.tif')\n",
    "input_dir = os.path.join(base_dir, 'Testbig_DEM.tif')\n",
    "output_dir = os.path.join(base_dir, 'Testbig_pymexhat_chunkIC.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement incremental writing technique to handle large GeoTIFF\n",
    "To handle large GeoTIFF files (>3.7 GB) without running into RAM issues, this version of code implements incremental reading and writing technique, involving processing the data in smaller chunks rather than loading the entire file into memory at once. This method should alleviate the RAM issues by never loading the entire DEM into memory. However, the performance could be impacted (required much more processing time) due to the overhead of reading and writing in chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_dask(input_dir, output_dir, a, dx, overlap):\n",
    "    \"\"\"\n",
    "    Processes a GeoTIFF file using Dask to manage large datasets and applies a wavelet transform.\n",
    "\n",
    "    Parameters:\n",
    "    input_dir : str\n",
    "        Input directory of the GeoTIFF file.\n",
    "    output_dir : str\n",
    "        Output directory for the processed GeoTIFF file.\n",
    "    a : float\n",
    "        Scale of the wavelet.\n",
    "    dx : float\n",
    "        Grid spacing in the input data.\n",
    "    overlap : int\n",
    "        Overlap size for chunk processing to mitigate edge effects.\n",
    "    \"\"\"\n",
    "    with rasterio.open(input_dir) as src:\n",
    "        meta = src.meta.copy()\n",
    "        dem = da.from_array(src.read(1), chunks=(512, 512))\n",
    "        \n",
    "        processed_data = dem.map_overlap(\n",
    "            lambda block: np.abs(conv2_mexh_fft(block, a, dx)[0]),\n",
    "            depth=overlap,\n",
    "            boundary='reflect',\n",
    "            trim=True,\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    meta.update(dtype=rasterio.float32, count=1, compress='lzw', bigtiff='IF_SAFER') #with compression, use more RAM\n",
    "    #meta.update(dtype=rasterio.float32, count=1, bigtiff='IF_SAFER')\n",
    "    \n",
    "    with rasterio.open(output_dir, 'w', **meta) as dst:\n",
    "        with TqdmCallback(desc=\"Processing\"):\n",
    "            result = processed_data.compute()\n",
    "\n",
    "        dst.write(result, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the DEM file\n",
    "with rasterio.open(input_dir) as src:\n",
    "    dem = src.read(1)\n",
    "    transform = src.transform\n",
    "    crs = src.crs\n",
    "\n",
    "    # Print the CRS information\n",
    "    print(f\"CRS: {crs}\")\n",
    "    print(f\"CRS as WKT: {crs.wkt}\")\n",
    "    print(f\"CRS as PROJ string: {crs.to_proj4()}\")\n",
    "    print(f\"CRS as EPSG code: {crs.to_epsg()}\")\n",
    "    print(f\"CRS as dictionary: {crs.to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = 1.8288  # Grid spacing\n",
    "a = 4.1  # Scale of the wavelet\n",
    "\n",
    "# Define overlap size\n",
    "overlap = int(a * 6)  # Example overlap based on wavelet scale 'a'\n",
    "#overlap = 50  # define arbitrary size of overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Process the GeoTIFF file\n",
    "    process_with_dask(input_dir, output_dir, a, dx, overlap) \n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Execution time: {end_time - start_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trim off the edges to remove artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-open the written file to trim edges\n",
    "with rasterio.open(output_dir, 'r+') as dst:\n",
    "    data = dst.read(1)  # Read the data to trim edges\n",
    "\n",
    "    # Calculate fringe value for trimming\n",
    "    fringeval = int(np.ceil(a * 4))\n",
    "    \n",
    "    # Apply NaNs to the edges to mask edge effects\n",
    "    data[:fringeval, :] = np.nan\n",
    "    data[:, :fringeval] = np.nan\n",
    "    data[-fringeval:, :] = np.nan\n",
    "    data[:, -fringeval:] = np.nan\n",
    "\n",
    "    # Overwrite the file with the trimmed data\n",
    "    dst.write(data, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
