{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mexican Hat Wavelet Analysis For Topographic Roughness\n",
    "* The original MATLAB code was developed from Dr. Adam M. Booth (Portland State Univeristy).  \n",
    "    * Citations:  \n",
    "        * Booth, A.M., Roering, J.J., Perron, J.T., 2009. Automated landslide mapping using spectral analysis and high-resolution topographic data: Puget Sound lowlands, Washington, and Portland    Hills, Oregon. Geomorphology 109, 132-147. https://doi.org/10.1016/j.geomorph.2009.02.027  \n",
    "        * Booth, A.M., LaHusen, S.R., Duvall, A.R., Montgomery, D.R., 2017. Holocene history of deep-seated landsliding in the North Fork Stillaguamish River valley from surface roughness analysis, radiocarbon dating, and numerical landscape evolution modeling. Journal of Geophysical Research: Earth Surface 122, 456-472. https://doi.org/10.1002/2016JF003934  \n",
    "* This MATLAB code was later adapted and revised by Dr. Sean R. LaHusen & Erich N. Herzig (Univeristy of Washington)\n",
    "    * Citations:  \n",
    "       * LaHusen, S.R., Duvall, A.R., Booth, A.M., Montgomery, D.R., 2016. Surface roughness dating of long-runout landslides near Oso, Washington (USA), reveals persistent postglacial hillslope instability. Geology 44, 111-114. https://doi.org/10.1130/G37267.1  \n",
    "       * LaHusen, S.R., Duvall, A.R., Booth, A.M., Grant, A., Mishkin, B.A., Montgomery, D.R., Struble, W., Roering, J.J., Wartman, J., 2020. Rainfall triggers more deep-seated landslides than Cascadia earthquakes in the Oregon Coast Range, USA. Science Advances 6, eaba6790. https://doi.org/10.1126/sciadv.aba6790  \n",
    "       * Herzig et al. (2023 in print) Bulletin of the Seismological Society of America. (details TBA)  \n",
    "* In November, 2023; this is code translated and optimized into this python version by Dr. Larry Syu-Heng Lai (Univeristy of Washington)  \n",
    "    * Citations: TBA  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "import rasterio\n",
    "from rasterio.enums import Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read a DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "input_tif_path = '/Users/larryslai/Library/CloudStorage/Dropbox/QGIS/WA LiDAR/cropped swWA DEM.tif'\n",
    "output_tif_path = '/Users/larryslai/Library/CloudStorage/Dropbox/QGIS/WA LiDAR/cropped_swWA_mexhat_py.tif'\n",
    "\n",
    "# Read the input GeoTIFF\n",
    "with rasterio.open(input_tif_path) as src:\n",
    "    dem = src.read(1)  # Read the first band into a 2D array\n",
    "    meta = src.meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See coordinate system info of the GeoTIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRS: EPSG:32149\n",
      "CRS as WKT: PROJCS[\"NAD83 / Washington South\",GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4269\"]],PROJECTION[\"Lambert_Conformal_Conic_2SP\"],PARAMETER[\"latitude_of_origin\",45.3333333333333],PARAMETER[\"central_meridian\",-120.5],PARAMETER[\"standard_parallel_1\",47.3333333333333],PARAMETER[\"standard_parallel_2\",45.8333333333333],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32149\"]]\n",
      "CRS as PROJ string: +init=epsg:32149\n",
      "CRS as EPSG code: 32149\n",
      "CRS as dictionary: {'init': 'epsg:32149'}\n"
     ]
    }
   ],
   "source": [
    "# Open the GeoTIFF file\n",
    "with rasterio.open(input_tif_path) as src:\n",
    "    # Read the CRS\n",
    "    crs = src.crs\n",
    "    \n",
    "    # Print the CRS information\n",
    "    print(f\"CRS: {crs}\")\n",
    "    print(f\"CRS as WKT: {crs.wkt}\")\n",
    "    print(f\"CRS as PROJ string: {crs.to_proj4()}\")\n",
    "    print(f\"CRS as EPSG code: {crs.to_epsg()}\")\n",
    "    print(f\"CRS as dictionary: {crs.to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mexican Hat Wavelet Analysis Function (original slower ver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2_mexh(dem, a, dx):\n",
    "    \"\"\"\n",
    "    Perform the 2D Continuous Wavelet Transform using the Mexican Hat wavelet.\n",
    "    \n",
    "    :param dem: Digital elevation model (2D numpy array).\n",
    "    :param a: Wavelet scale.\n",
    "    :param dx: Grid spacing.\n",
    "    :return: Tuple of (C, frq, wave), the wavelet coefficients and frequencies.\n",
    "    \"\"\"\n",
    "    # Generate the Mexican Hat wavelet kernel at wavelet scale a\n",
    "    sz = int(np.ceil(8 * a))  # Kernel size\n",
    "    X, Y = np.meshgrid(np.arange(-sz, sz+1), np.arange(-sz, sz+1))\n",
    "\n",
    "    # Scaled Mexican Hat wavelet (psi)\n",
    "    psi = (-1 / (np.pi * (a * dx)**4)) * (1 - (X**2 + Y**2) / (2 * a**2)) * np.exp(-(X**2 + Y**2) / (2 * a**2))\n",
    "\n",
    "    # Convolve dem with psi\n",
    "    C = dx**2 * scipy.signal.convolve2d(dem * 0.3048, psi, mode='same')\n",
    "\n",
    "    # Mask edge effects with NaN values\n",
    "    fringeval = int(np.ceil(a * 4))\n",
    "    C[:fringeval, :] = np.nan\n",
    "    C[-fringeval:, :] = np.nan\n",
    "    C[:, :fringeval] = np.nan\n",
    "    C[:, -fringeval:] = np.nan\n",
    "\n",
    "    # Frequency and wavelength calculations\n",
    "    wave = 2 * np.pi * dx * a / np.sqrt(5 / 2)  # Wavelength\n",
    "    frq = 1 / wave  # Frequency\n",
    "    \n",
    "    return C, frq, wave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 4.1  # Wavelet scale\n",
    "dx = 1.8288  # Grid spacing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C, frq, wave = conv2_mexh(dem, a, dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output data into GeoTIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define georeference system\n",
    "coord_ref_sys_code = 32149 #NAD38 Washington South\n",
    "# coord_ref_sys_code = 32610 #WGS84_UTM Zone 10N\n",
    "\n",
    "# Prepare the metadata for writing the output GeoTIFF\n",
    "meta.update({\n",
    "    'dtype': 'float32',\n",
    "    'nodata': np.nan,\n",
    "    'crs': f'EPSG:{coord_ref_sys_code}'\n",
    "})\n",
    "\n",
    "# Write the result to a new GeoTIFF\n",
    "with rasterio.open(output_tif_path, 'w', **meta) as dst:\n",
    "    dst.write(C.astype(np.float32), 1)  # Write the computed C as the first band"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mexican Hat Wavelet Analysis Function - Optimized using Fast Fourier Transform (FFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use FFT convolution for better performance on large arrays\n",
    "def conv2_mexh_fft(dem, a, dx):\n",
    "    \"\"\"\n",
    "    Perform the 2D Continuous Wavelet Transform using the Mexican Hat wavelet.\n",
    "    \n",
    "    :param dem: Digital elevation model (2D numpy array).\n",
    "    :param a: Wavelet scale.\n",
    "    :param dx: Grid spacing.\n",
    "    :return: Tuple of (C, frq, wave), the wavelet coefficients and frequencies.\n",
    "    \"\"\"\n",
    "    # Kernel size, assuming the wavelet decays to 0 at the edges\n",
    "    sz = int(np.ceil(8 * a))  \n",
    "    X, Y = np.meshgrid(np.arange(-sz, sz+1), np.arange(-sz, sz+1))\n",
    "\n",
    "    # Scaled Mexican Hat wavelet (psi)\n",
    "    psi = (-1 / (np.pi * (a * dx)**4)) * (1 - (X**2 + Y**2) / (2 * a**2)) * np.exp(-(X**2 + Y**2) / (2 * a**2))\n",
    "\n",
    "    # Convolve dem with psi using FFT for speed optimization\n",
    "    C = scipy.signal.fftconvolve(dem, psi, mode='same')\n",
    "\n",
    "    # Frequency and wavelength calculations\n",
    "    wave = 2 * np.pi * dx * a / np.sqrt(5 / 2)  # Wavelength\n",
    "    frq = 1 / wave  # Frequency\n",
    "    \n",
    "    return C, frq, wave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 4.1  # Wavelet scale\n",
    "dx = 1.8288  # Grid spacing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Large DEM to be processed in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a suitable chunk size depending on your system's memory\n",
    "chunk_size = 20000  # Example size, adjust based on your system's capability\n",
    "\n",
    "# Initialize an empty array to store the wavelet coefficients\n",
    "C_full = np.empty(dem.shape, dtype=np.float32)\n",
    "\n",
    "# Perform the wavelet transform in chunks to avoid memory issues\n",
    "for i in range(0, dem.shape[0], chunk_size):\n",
    "    for j in range(0, dem.shape[1], chunk_size):\n",
    "        # Extract the chunk of the DEM to process\n",
    "        dem_chunk = dem[i:i+chunk_size, j:j+chunk_size]\n",
    "\n",
    "        # Perform the wavelet transform on the chunk\n",
    "        C_chunk, frq, wave = conv2_mexh_fft(dem_chunk, a, dx)\n",
    "\n",
    "        # Store the result in the appropriate part of the full array\n",
    "        C_full[i:i+chunk_size, j:j+chunk_size] = C_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perform the wavelet transform on the entire dataset (for smaller DEM file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "C_full, frq, wave = conv2_mexh_fft(dem, a, dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output data into GeoTIFF (Optimzation made for faster writing)    \n",
    "* Enabling geotiff compression to reduce writing time\n",
    "* Allowing Tile-based writing if needed (disabled for now)\n",
    "* Enabling parallel writing - writing data into chunks/blocks  \n",
    "* Enabling BIGTIFF parameter to allow writing a large GeoTIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define georeference system\n",
    "coord_ref_sys_code = 32149 #NAD38 Washington South\n",
    "#coord_ref_sys_code = 32610 #WGS84_UTM Zone 10N\n",
    "\n",
    "# Prepare the metadata for writing the output GeoTIFF\n",
    "meta.update({\n",
    "    'dtype': 'float32',\n",
    "    'nodata': np.nan,\n",
    "    'crs': f'EPSG:{coord_ref_sys_code}',\n",
    "    'compress': 'lzw',  # Using LZW compression\n",
    "    'tiled': True,      # Writing in tiles\n",
    "    'blockxsize': 256,  # Block size (adjust as needed)\n",
    "    'blockysize': 256,\n",
    "    'BIGTIFF': 'YES'    # Explicitly use BigTIFF format\n",
    "})\n",
    "\n",
    "# Write the result to a new GeoTIFF\n",
    "with rasterio.open(output_tif_path, 'w', **meta) as dst:\n",
    "    dst.write(C_full, 1)  # Write the computed C_full as the first band"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
